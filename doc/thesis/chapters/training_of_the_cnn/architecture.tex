\section{Architecture}
\label{sec:training_of_the_cnn:architecture}

Designing a \acrlong{cnn} architecture from scratch requires choosing the types of layers and their arrangement as well as many hyperparameters.
For this reason, a lot of trial an error is involved in the design process of an adequate model \cite{training_arch_design}.
There are, however, certain design principles that work really well \cite{training_arch_hyper}:

\begin{enumerate}
  \item Starting with a low number of filters (high-level feature detection)
  \item Increasing the number of filters towards the end (low-level feature detection)
  \item Decreasing the spatial dimensions of the feature maps towards the end
  \item Using kernel sizes of $3\times 3$, $5\times 5$ or $7\times 7$ for convolutional layers
  \item Using pool sizes of $2\times 2$ or $3\times 3$ with a stride of two for max-pooling layers
  \item Adding additional layers until the model is overfitting
  \item Using state-of-the-art networks as inspiration
\end{enumerate}

A summary of all the layers of the final \acrshort{cnn} architecture is listed in table \ref{tab:arch}.
The first convolutional layer uses only \num{16} filters and a larger kernel size of $5\times 5$ due to the large dimensions of the input images.
All other convolutional layers use a kernel size of $3\times 3$ while steadily increasing the number of used filters up to \num{128}.
The designed architecture uses six max-pooling layers with a pool size of $2\times 2$ and a stride of two.
This reduces the spatial dimensions of the feature maps to $4\times 5$ (height $\times$ width).
The output of the fully-connected layer \texttt{fc8} is about the size of the output layer \texttt{fc9} squared.
This allows the output layer to combine many of the different high-level features to create a confident prediction.

Even though the model was not yet overfitting, no additional layers were added.
The reason for this is that with the current architecture the classification performance is already exceptional.

Furthermore, the number of trainable parameters (weights) is relative low compared to state-of-the-art networks like VGG, ResNet or Inception \cite{training_arch_keras}.
This increases the throughput considerably, as less mathematical operations are required.
The key to keeping the total number of trainable parameter low is evident when analyzing table \ref{tab:arch}.
A whopping \num{1311232} of the \num{1614486} total weights can be attributed to the connection between the feature maps of the last convolutional layer \texttt{conv7} and the artificial output neurons of the first dense layer \texttt{fc8}.
This accounts for \SI{81.22}{\percent} of all trainable parameters.
For this reason the spatial dimensions of the feature maps should be decreased towards the end.

\begin{table}
  \caption{Layers of the \acrshort{cnn} architecture}
  \label{tab:arch}
  \centering
  \begin{tabular}{lllllll}
    \toprule
    \textbf{Layer} & \textbf{Type} & \textbf{Activation} & \textbf{Filters} & \textbf{Kernel} & \textbf{Output Shape} & \textbf{Param \#} \\
    \midrule
    \textbf{conv1} & \texttt{Conv2D} & \acrshort{relu} & \num{16} & $5\times 5$ & $256\times 320\times 16$ & \num{1216} \\
    \textbf{pool1} & \texttt{MaxPooling2D} &  &  & $2\times 2$ & $128\times 160\times 16$ & \num{0} \\
    \midrule
    \textbf{conv2} & \texttt{Conv2D} & \acrshort{relu} & \num{32} & $3\times 3$ & $128\times 160\times 32$ & \num{4640} \\
    \textbf{pool2} & \texttt{MaxPooling2D} &  &  & $2\times 2$ & $64\times 80\times 32$ & \num{0} \\
    \midrule
    \textbf{conv3} & \texttt{Conv2D} & \acrshort{relu} & \num{32} & $3\times 3$ & $64\times 80\times 32$ & \num{9248} \\
    \textbf{pool3} & \texttt{MaxPooling2D} &  &  & $2\times 2$ & $32\times 40\times 32$ & \num{0} \\
    \midrule
    \textbf{conv4} & \texttt{Conv2D} & \acrshort{relu} & \num{64} & $3\times 3$ & $32\times 40\times 64$ & \num{18496} \\
    \textbf{pool4} & \texttt{MaxPooling2D} &  &  & $2\times 2$ & $16\times 20\times 64$ & \num{0} \\
    \midrule
    \textbf{conv5} & \texttt{Conv2D} & \acrshort{relu} & \num{64} & $3\times 3$ & $16\times 20\times 64$ & \num{36928} \\
    \textbf{pool5} & \texttt{MaxPooling2D} &  &  & $2\times 2$ & $8\times 10\times 64$ & \num{0} \\
    \midrule
    \textbf{conv6} & \texttt{Conv2D} & \acrshort{relu} & \num{128} & $3\times 3$ & $8\times 10\times 128$ & \num{73856} \\
    \textbf{pool6} & \texttt{MaxPooling2D} &  &  & $2\times 2$ & $4\times 5\times 128$ & \num{0} \\
    \midrule
    \textbf{conv7} & \texttt{Conv2D} & \acrshort{relu} & \num{128} & $3\times 3$ & $4\times 5\times 128$ & \num{147584} \\
    \midrule
    \textbf{flatten} & \texttt{Flatten} &  &  &  & \num{2560} & \num{0} \\
    \textbf{fc8} & \texttt{Dense} & \acrshort{relu} &  &  & \num{512} & \num{1311232} \\
    \textbf{fc9} & \texttt{Dense} &  &  &  & \num{22} & \num{11286} \\
    \bottomrule
  \end{tabular}
\end{table}

% ------------------------------------------------------------------------------------------------------------------------------
\subsection{Visualization}
\label{subsec:training_of_the_cnn:architecture:visualization}
Figure \ref{fig:arch} visualizes the final architecture of the \acrshort{cnn} model.
The visualization was created with Ti\textit{k}Z and the help of the open-source repository \textit{PlotNeuralNetwork} \cite{training_arch_plot}.

The boxes represent the outputs of the different layers.
On the one hand, the light orange boxes represent the feature maps of the convolutional layers and, on the other hand the purple boxes represent the artificial output neurons of the dense layers.
The darker colored bands on the boxes indicate that the \acrshort{relu} activation function is applied.
The red boxes represent max-pooling layers, which decrease the spatial dimensions.
The dashed lines between the output of \texttt{conv7} and \texttt{fc8} depict the flattening in addition to the dense connection.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{arch}
  \caption{Final architecture of the \acrlong{cnn}}
  \label{fig:arch}
\end{figure}

% ------------------------------------------------------------------------------------------------------------------------------
\subsection{Implementation}
\label{subsec:training_of_the_cnn:architecture:implementation}
The \acrshort{cnn} architecture is defined in the Python script \texttt{cnn.py}, as shown in listing \ref{lst:arch}.
The script uses the open-source software library TensorFlow v2.2.0, along with the high-level Keras \acrshort{api} implemented in the \texttt{tf.keras} module \cite{training_arch_tf_keras}.
To define the architecture a \texttt{Sequential} model is used to arrange the desired layers in a plain stack \cite{training_arch_tf_keras_seq}.

\begin{lstlisting}[style=python, caption={Sequential model}, label=lst:arch]
# Convolutional Neural Network Architecture
# Convolution layers
model = models.Sequential()
model.add(layers.Conv2D(16, (5, 5), padding='same', activation='relu', input_shape=fh.inf_shape))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))

# Dense layers
model.add(layers.Flatten())
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(22))
\end{lstlisting}
