\section{Artificial Neural Network}
\label{sec:theoretical_background:ann}

An \acrlong{ann} is a model used in \acrshort{ml} which consists of artificial neurons and weighted connections between those neurons.
They usually feature an input layer, several hidden layers and an output layer.
The individual layers consist of an arbitrary number of artificial neurons, which are connected to one another \cite[p.~33-36]{nn}.

The simplest \acrshort{ann} is the feedforward neural network, where the information propagates in only one direction.
A common propagation function is the weighted sum of the neurons from the previous layer.
Therefore, the output of each neuron $i$ is multiplied with the weight $w_{i, j}$ associated to the connection between the two neurons.
These values are summed up and added to a bias term.
In a last step, an activation function is applied, which results in equation \ref{eq:artificial_neuron}.

\begin{equation}
  y_j = a\left(\sum\limits_{i} x_i \cdot w_{i, j} + b\right)
  \label{eq:artificial_neuron}
\end{equation}

where

\begin{tabular}{lll}
  $y_j$ & = & output of neuron $j$ \\
  $a(.)$ & = & activation function \\
  $x_i$ & = & output of neuron $i$ from the previous layer \\
  $w_{i, j}$ & = & weight $i, j$ connecting the output of neuron $i$ with the input of neuron $j$ \\
  $b$ & = & bias \\
\end{tabular}
\\




\subsection{Activation Function}
\label{subsec:theoretical_background:ann:activation_function}

An activation function is responsible for transforming the input of an artificial neuron to its output.
There exists a variety of different linear and non-linear activation functions.
However, non-linear activation functions are preferred due to their ability to learn more complex structures in the data.
The sigmoid and hyperbolic tangent activation functions were traditionally very popular, but they both suffer from a saturation problem.
Currently, the \acrfull{relu} is the most commonly used activation function in the field of deep learning.
It is a piecewise-defined function, which outputs the input if it positive and zero otherwise.
This is shown in equation \ref{eq:relu}.
Furthermore, the \acrshort{relu} activation function requires only a single comparison and is therefore not very computationally expensive \cite{relu}.

\begin{equation}
  a(x) = x^+ = \max(0, x)
  \label{eq:relu}
\end{equation}
